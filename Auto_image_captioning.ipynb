{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auto image captioning",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2j2enO-Sc16"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPWcteV-TQlG"
      },
      "source": [
        "!kaggle datasets download -d shweta2407/flickr8k-imageswithcaptions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPlT2BM2UGQt"
      },
      "source": [
        "!unzip flickr8k-imageswithcaptions.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCOE6JtgUyWv"
      },
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "from IPython.display import Image\n",
        "from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXSXLgZYVEZH"
      },
      "source": [
        "image_path = 'Flickr8k_Dataset/Flicker8k_Dataset'\n",
        "caption_path = 'Flickr8k_text/Flickr8k.token.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4Rx3IrhVN5n"
      },
      "source": [
        "def load_captions(path):\n",
        "    captions_dict = {}    \n",
        "    for caption in open(path):\n",
        "        tokens = caption.split()\n",
        "        caption_id, caption_text = tokens[0].split('.')[0], tokens[1:]\n",
        "        caption_text = ' '.join(caption_text)\n",
        "        if caption_id not in captions_dict:\n",
        "            captions_dict[caption_id] = caption_text\n",
        "        \n",
        "    return captions_dict\n",
        "\n",
        "captions_dict = load_captions(caption_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqPB6iU0VW4X"
      },
      "source": [
        "print(string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB9p-0FrazpR"
      },
      "source": [
        "new_captions_dict = {}\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "for caption_id, caption_text in captions_dict.items():\n",
        "    caption_text = caption_text.split()\n",
        "    caption_text = [token.lower() for token in caption_text]\n",
        "    caption_text = [token.translate(table) for token in caption_text]\n",
        "    caption_text = [token for token in caption_text if len(token)>1]\n",
        "    # store cleaned captions\n",
        "    new_captions_dict[caption_id] = 'startseq ' + ' '.join(caption_text) + ' endseq'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mBEAEHGcjl8"
      },
      "source": [
        "caption_images_list = []\n",
        "image_index = list(new_captions_dict.keys())\n",
        "caption_images_list = [ image.split('.')[0] for image in os.listdir(image_path) if image.split('.')[0] in image_index ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US0Su-g6cxcz"
      },
      "source": [
        "train_validate_images = caption_images_list[0:8081] \n",
        "test_images = caption_images_list[8081:8091]\n",
        "test_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skRkFQSUlH6q"
      },
      "source": [
        "def extract_features(directory, image_keys):\n",
        "    model = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')\n",
        "    print(model.summary())\n",
        "    features = dict()\n",
        "    \n",
        "    for name in image_keys:\n",
        "        filename = directory + '/' + name + '.jpg'\n",
        "        image = load_img(filename, target_size=(224, 224))\n",
        "        image = img_to_array(image)\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "        image = preprocess_input(image)\n",
        "        feature = model.predict(image, verbose=0)\n",
        "        image_id = name.split('.')[0]\n",
        "        # store feature\n",
        "        features[image_id] = feature\n",
        "         \n",
        "\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeQtwS77yT1J"
      },
      "source": [
        "train_validate_features1 = extract_features(image_path, train_validate_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afo_twLuyk3h"
      },
      "source": [
        "print(\"{} : {}\".format(list(train_validate_features1.keys())[0], train_validate_features1[list(train_validate_features1.keys())[0]] ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdH_LVORDnLu"
      },
      "source": [
        "len(train_validate_features1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I2kvL6SDnli"
      },
      "source": [
        "dump(train_validate_features1, open('./train_validate_features1.pkl', 'wb'))\n",
        "# train_validate_features1 = load(open('./train_validate_features1.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJmOEIsODni7"
      },
      "source": [
        "# make a dictionary of image with caption for train_validate_images\n",
        "train_validate_image_caption = {}\n",
        "\n",
        "for image, caption in new_captions_dict.items():\n",
        "    if image in train_validate_images and image in list(train_validate_features1.keys()):\n",
        "        train_validate_image_caption.update({image : caption})\n",
        "\n",
        "len(train_validate_image_caption)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yRA771IDnga"
      },
      "source": [
        "print(list(train_validate_image_caption.values())[2])\n",
        "Image(image_path+'/'+list(train_validate_image_caption.keys())[2]+'.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Oqp1ljXDnd6"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(list(train_validate_image_caption.values()))\n",
        "vocab_len = len(tokenizer.word_index) + 1\n",
        "max_len = max(len(train_validate_image_caption[image].split()) for image in train_validate_image_caption)\n",
        "\n",
        "print(\"vocab_len \", vocab_len)\n",
        "print(\"max_len \", max_len)\n",
        "\n",
        "def prepare_data(image_keys):\n",
        "    # x1 will store  image feature, x2 will store one sequence and y will store the next sequence\n",
        "    x1, x2, y = [], [], []\n",
        "    for image in image_keys:\n",
        "      caption = train_validate_image_caption[image]\n",
        "      caption = caption.split()\n",
        "      seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "      # print(seq)\n",
        "      length = len(seq)\n",
        "\n",
        "      for i in range(1, length):\n",
        "        x2_seq, y_seq = seq[:i] , seq[i] \n",
        "        # print(y_seq) \n",
        "        x2_seq = pad_sequences([x2_seq], maxlen = max_len)[0]\n",
        "        y_seq = to_categorical([y_seq], num_classes = vocab_len)[0]\n",
        "        # print(y_seq) \n",
        "        x1.append( train_validate_features1[image][0] )\n",
        "        x2.append(x2_seq)\n",
        "        y.append(y_seq)\n",
        "                \n",
        "    return np.array(x1), np.array(x2), np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aYzW3kJVRgW"
      },
      "source": [
        "train_x1, train_x2, train_y = prepare_data( train_validate_images[0:7081] )\n",
        "validate_x1, validate_x2, validate_y = prepare_data( train_validate_images[7081:8081] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7UmI3hmW_0z"
      },
      "source": [
        "embedding_size = 128\n",
        "image_model = Sequential()\n",
        "\n",
        "image_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\n",
        "image_model.add(Dropout(0.5))\n",
        "image_model.add(RepeatVector(max_len))\n",
        "\n",
        "image_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXSS3z55X5JQ"
      },
      "source": [
        "language_model = Sequential()\n",
        "\n",
        "language_model.add(Embedding(input_dim=vocab_len, output_dim=embedding_size, input_length=max_len))\n",
        "language_model.add(LSTM(256,return_sequences=True))\n",
        "language_model.add(Dropout(0.5))\n",
        "language_model.add(TimeDistributed(Dense(embedding_size)))\n",
        "\n",
        "language_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEbiMh8KfVyG"
      },
      "source": [
        "conca = Concatenate()([image_model.output, language_model.output])\n",
        "x = LSTM(128, dropout=0.5, recurrent_dropout=0.5,return_sequences=True)(conca)\n",
        "x = LSTM(512, dropout=0.5, recurrent_dropout=0.5,return_sequences=False)(x)\n",
        "x = Dense(vocab_len)(x)\n",
        "out = Activation('softmax')(x)\n",
        "model = Model(inputs=[image_model.input, language_model.input], outputs = out)\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73q0upDTgmYa"
      },
      "source": [
        "import tensorflow\n",
        "filepath = './image_captioning.h5'\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 15:\n",
        "    return lr\n",
        "  if epoch < 30 and epoch > 15:\n",
        "    return lr/100 \n",
        "  else:\n",
        "    return lr/10000\n",
        "callbacks = [ ModelCheckpoint(filepath= filepath, verbose = 2,save_best_only=True, monitor='val_loss', mode='min'),tensorflow.keras.callbacks.LearningRateScheduler(scheduler) ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU70plnGhaS8"
      },
      "source": [
        "print(\"shape of train_x1 \", train_x1.shape)\n",
        "print(\"shape of train_x2 \", train_x2.shape)\n",
        "print(\"shape of train_y \", train_y.shape)\n",
        "print()\n",
        "print(\"shape of validate_x1 \", validate_x1.shape)\n",
        "print(\"shape of validate_x2 \", validate_x2.shape)\n",
        "print(\"shape of validate_y \", validate_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q6HGqEahaPE"
      },
      "source": [
        "plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iATIQ4oKhaF8"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "EPOCHS = 50\n",
        "history = model.fit([train_x1, train_x2],  \n",
        "                    train_y,              \n",
        "                    verbose = 1,            \n",
        "                    epochs = EPOCHS,\n",
        "                    batch_size = BATCH_SIZE,\n",
        "                    callbacks = callbacks, \n",
        "                    validation_data=([validate_x1, validate_x2], validate_y)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-xruOIk1le"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_loss','val_loss'], loc = 'upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9KtpHQs9iP_"
      },
      "source": [
        "def extract_feat_single(filename):\n",
        "    model = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')\n",
        "    image = load_img(filename, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    image = preprocess_input(image)\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    return feature\n",
        "\n",
        "def word_for_id(integer, tokenizr):\n",
        "    for word, index in tokenizr.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XoCUBFb96sG"
      },
      "source": [
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baRGdQS9-nD_"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GWLXk3wSwZd"
      },
      "source": [
        "model = load_model('./image_captioning.h5')\n",
        "tokenizr = Tokenizer()\n",
        "tokenizr.fit_on_texts([caption for image, caption in new_captions_dict.items() if image in train_validate_images])\n",
        "max_length = max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tB3k9a9-pGQ"
      },
      "source": [
        "photo = extract_feat_single('Flickr8k_Dataset/Flicker8k_Dataset/554526471_a31f8b74ef.jpg')  \n",
        "\n",
        "in_text = 'startseq'\n",
        "for i in range(max_length):\n",
        "    sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "    yhat = model.predict([photo,sequence], verbose=0)\n",
        "    yhat = np.argmax(yhat)\n",
        "    word = word_for_id(yhat, tokenizer)\n",
        "    if word is None:\n",
        "        break\n",
        "    in_text += ' ' + word\n",
        "    if word == 'endseq':\n",
        "        break\n",
        "in_text = in_text.replace('startseq','') \n",
        "in_text = in_text.replace('endseq','') \n",
        "print(\"Predicted caption -> \", in_text)\n",
        "Image('Flickr8k_Dataset/Flicker8k_Dataset/554526471_a31f8b74ef.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLN4K5eHLl6n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}